<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="chapter-MapReduce_Internals" xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:svg="http://www.w3.org/2000/svg"
         xmlns:m="http://www.w3.org/1998/Math/MathML"
         xmlns:html="http://www.w3.org/1999/xhtml"
         xmlns:db="http://docbook.org/ns/docbook">

    <title>How MapReduce Works -- The Internals</title>
    <para>
        This chapter looks 'under the hood' of Map Reduce and explains how things work.
    </para>
    <sect1>
        <title>Job Submission Process</title>
        <para>
            We already described a map reduce job.  Now let's look "under the hood" at what happens when we run a job.  There is actually quite a bit going on behind the scenes.  Lets start with an overall submission process diagram.  Note that this covers classic Map Reduce or MRV1.
        </para>
        <figure id="mapreduce-job-submission">
            <title>Map Reduce Job Submission Process - Hadoop 1</title>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="mr_job_submission.jpg" format="JPG" scale="100" />
                </imageobject>
            </mediaobject>
        </figure>
        <para>
            Whoa, that is quite a lot; let's take it in stride
        </para>
        <sect2>
            <title>Client Side : Step 1) Job Submission </title>
            <para>
                Map Reduce jobs are submitted by a client.
                <note>
                    You don't need to login to a Hadoop cluster to submit a job.  Actually in most cases Hadoop admins won't allow users to login to the cluster.  Gateway machines would be set up that have access to the Hadoop cluster.  Users would login to gateway machines and submit their jobs.
                </note>
            </para>

            <sect2>
                <title>Step 2) Getting JobID</title>
                <para>
                    Each Map Reduce job gets assigned a unique ID. Clients contact the JobTracker to grab a new jobID.
                </para>
            </sect2>


            <sect2>
                <title>Step 3) Copying job resources to HDFS</title>
                <para>
                    Before running Map Reduce jobs on cluster nodes, we need to distribute a bunch of things to the nodes themselves  (Remember code to data concept?).  Following are some artifacts that get distributed.
                </para>
                <itemizedlist>
                    <listitem>
                        The Map Reduce program jar file, which contains the compiled Java code.
                    </listitem>
                    <listitem>
                        Any other dependent jar files that are used by Map Reduce job.
                    </listitem>
                    <listitem>
                        Any other files.
                    </listitem>
                </itemizedlist>
            </sect2>

            <sect2>
                <title>Step 4) Submit the job</title>
                <para>
                    The client submits the job to JobTracker
                </para>
            </sect2>
        </sect2>

        <sect2>
            <title>JobTracker side : Getting the Job going</title>

            <sect2>
                <title>Step 6) Job Splits</title>
                <para>
                    One important concept of Map Reduce is parallelism.  The input is divided up and processed across the cluster.   Inputs are chopped up into what are called 'input splits'.  JobTracker next calculates input splits.
                </para>
            </sect2>

            <sect2>
                <title>Step 7) Job is kicked off</title>
                <para>
                    JobTracker starts the job and TaskTrackers will start executing the job.
                </para>
            </sect2>
        </sect2>

        <sect2>
            <title>TaskTracker side : Executing the Job</title>
            <para>
                JobTracker doesn't run any jobs.  TaskTrackers actually execute the jobs.
            </para>
            <sect2>
                <title>Step 8) Getting Job Resources locally</title>
                <para>
                    Remember the job resources that were copied to HDFS by the client?  The TaskTracker nodes now need to copy these resources locally.  These files are copied from HDFS to local storage.  A 'local run directory' is created and the contents are copied there and 'un jarred'.
                </para>
            </sect2>

            <sect2>
                <title>Step 9) Spinning up the map or reduce job</title>
                <para>
                    Now that all job resources are locally available, the actual job is ready to be run.  TaskTracker spins up a separate Java Virtual Machine (JVM) and runs map or reduce programs.   So why is map/reduce run in a separate VM?  This way any errors in map/reduce code don't kill the TaskTracker.
                </para>
            </sect2>

            <sect2>
                <title>Step 10) Heartbeats -- reporting back to JobTracker</title>
                <para>
                    TaskTracker periodically sends updates to JobTracker.  This is called the 'heartbeat'.  The Heartbeat contains information like job status, resource usage, etc.  JobTracker collects all reports from TaskTrackers and figures out how the job is progressing.
                </para>
            </sect2>
        </sect2>
    </sect1>

    <section>
        <title>Measuring Progress of Tasks</title>
        <para>
            Map Reduce tasks report their progress.  Tasks that don't report progress for a while can be marked as 'hung' and killed off.  So it is important for a task to keep updating its progress.  Progress is reported by the following activities:
            <itemizedlist>
                <listitem>
                    Reading an input record (progress updated automatically, no user intervention required).
                </listitem>
                <listitem>
                    Writing an output record (automatic).
                </listitem>
                <listitem>
                    Incrementing counters (manual, user has to do this explicitly in map reduce code).
                </listitem>
                <listitem>
                    Setting status message (manual).
                </listitem>
                <listitem>
                    Explicitly setting progress status (manual).
                </listitem>
            </itemizedlist>
        </para>
    </section>


    <section>
        <title>Dealing With Task Failures</title>
        <para>
            While running tasks on a cluster of commodity machines, some things are bound fail.  Machines fail,  disks go bad, etc.   Luckily Map Reduce is built to handle failures.
        </para>
        <para>
            How is a task failure detected?
        </para>
        <itemizedlist>
            <listitem>
                An exception thrown by a map or reduce program.  The child JVM notifies TaskTracker of this failure.  The exception is then propagated up the chain to JobTracker.
            </listitem>
            <listitem>
                The child JVM exits abnormally.  Say it runs out of memory, etc.
            </listitem>
            <listitem>
                <para>

                    Hanging tasks - at times map or reduce tasks become unresponsive.  It could be a long running task (crawling web sites) or stuck in a loop.  If a task hasn't updated it's progress in a while (10 minutes), then that task is marked as non-responsive and noted as failed.
                </para>
                <para>
                    This timeout interval can be set by a mapred.task.timeout configuration parameter.  If this is set to zero, then the timeouts are disabled (in other words Map Reduce doesn't mark the tasks as failed).
                </para>
            </listitem>
        </itemizedlist>
        <para>
            So what happens when a task fails?  The TaskTracker detects the failure and sends the failure up the chain to JobTracker.  JobTracker will retry this task.  The task is retried on another node other than where it failed once.  The reasoning is the node may be failing (hardware failure, etc.) and JobTracker wants to try the task somewhere else.
        </para>
        <para>
            A failed task is retried 3 times.  If it keeps failing, then the entire Job is marked failed.  A coding error (e.g. Null Pointer Exception) can make the task fail consistently, eventually failing the job.
        </para>
        <para>
            The maximum number of retries is controlled by 'mapred.map.max.attempts' and 'mapred.reduce.max.attempts'.
        </para>
    </section>

    <sect1>
        <title>Job Scheduling</title>
        <para>
            A large cluster may be shared with many users.  Users submit jobs and these jobs could be running on the cluster at the same time.  They will compete for resources, open map/reduce slots, etc.  Map Reduce schedulers will manage how this is orchestrated.  There are three schedulers that can be used.
        </para>
        <glosslist>
            <glossentry>
                <glossterm>
                    FIFO (First In First Out) Scheduler:
                </glossterm>
                <glossdef>
                    <para>
                        This is the default scheduler for Hadoop 1.  The concept is pretty simple.  Jobs from all users are slotted in to a queue.  And they are executed in the order they arrive.
                    </para>
                    <para>
                        FIFO has a serious problem in a multi-user environment.  Imagine that a job that will be running for 10 hours is currently running on the cluster.  Then, another user submits a quick job that would be done in 10 minutes.  If we stick to 'first come first served' the short job will be stuck behind the long running job for a very long time.
                    </para>
                </glossdef>
            </glossentry>
            <glossentry>
                <glossterm>
                    Fair Scheduler:
                </glossterm>
                <glossdef>
                    <para>
                        Fair scheduler was developed by Facebook.   It is designed to effectively share the cluster across various users.
                    </para>
                    <para>
                        This scheduler allocates cluster time 'fairly' between multiple users.  So in our previous example a short job will make progress along with the long-running job.
                    </para>
                    <para>
                        Fair Scheduler manages this by creating 'work pools' for each user.  Each user's task is queued in their own pool and the scheduler makes sure each pool receives enough execution time.  If there is excess capacity (open map slots or reduce slots) they are shared among the pools.
                    </para>
                </glossdef>
            </glossentry>
            <glossentry>
                <glossterm>
                    Capacity Scheduler:
                </glossterm>
                <glossdef>
                    <para>
                        Capacity Scheduler was developed by Yahoo.  It shares a lot of qualities with Fair Scheduler.   However, it provisions multiple users across the cluster using 'queues' instead of the pools used by Fair Scheduler.
                    </para>
                </glossdef>
            </glossentry>
        </glosslist>
    </sect1>

</chapter>
