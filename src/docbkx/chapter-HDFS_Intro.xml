<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="chapter-HDFS_Intro" xmlns="http://docbook.org/ns/docbook"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:svg="http://www.w3.org/2000/svg"
    xmlns:m="http://www.w3.org/1998/Math/MathML"
    xmlns:html="http://www.w3.org/1999/xhtml"
    xmlns:db="http://docbook.org/ns/docbook">

    <title>Hadoop Distributed File System (HDFS) -- Introduction</title>
    <para>
        In this chapter we will learn about the Hadoop Distributed File System, also known as HDFS.   When people say 'Hadoop' it usually includes two core components : HDFS and MapReduce
    </para>
    <para>
        HDFS is the 'file system' or 'storage layer' of Hadoop.  It takes care of storing data -- and it can handle very large amount of data (on a petabytes scale).
    </para>

    <para>
        In this chapter, we will look at the concepts of HDFS
    </para>

    <section>
        <title>HDFS Concepts</title>

        <section>
            <title>Problem :  Data is too big store in one computer</title>
            <para>
                Today's big data is 'too big' to store in ONE single computer -- no matter how powerful it is and how much storage it has.  This eliminates lot of storage system and databases that were built for single machines.  So we are going to build the system to run on multiple networked computers.  The file system will look like a unified single file system to the 'outside' world
            </para>
            <para>
                <bridgehead>Hadoop solution : Data is stored on multiple computers</bridgehead>
            </para>
            <!--
            <figure>
                <title>TODO : multi elephant picture</title>
                <graphic fileref="question2.jpeg"></graphic>
            </figure>
            -->
        </section>


        <section>
            <title>
                Problem : Very high end machines are expensive
            </title>
            <para>
                Now that we have decided that we need a cluster of computers, what kind of machines are they?  Traditional storage machines are expensive with top-end components, sometimes with 'exotic' components (e.g. fiber channel for disk arrays, etc).  Obviously these computers cost a pretty penny.
            </para>
            <figure>
                <title>Cray computer</title>
                <graphic fileref="cray_sm.jpg"></graphic>
            </figure>
            <para>
                We want our system to be cost-effective, so we are not going to use these 'expensive' machines.  Instead we will opt to use commodity hardware.  By that we don't mean cheapo desktop class machines.  We will use performant server class machines -- but these will be commodity servers that you can order from any of the vendors (Dell, HP, etc)
            </para>
            <para>
                So what do these server machines look like?  Look at the <xref linkend="chapter-Hardware_Software"/> guide.
            </para>

            <bridgehead>
                Hadoop solution : Run on commodity hardware
            </bridgehead>
        </section>

        <section>
            <title>
                Problem : Commodity hardware will fail
            </title>
            <para>
                In the old days of distributed computing, failure was an exception, and hardware errors were not tolerated well.  So companies providing gear for distributed computing made sure their hardware seldom failed.  This is achieved by using high quality components, and having backup systems (in come cases backup to backup systems!).  So the machines are engineered to withstand component failures, but still keep functioning.  This line of thinking created hardware that is impressive, but EXPENSIVE!
            </para>
            <para>
                On the other hand we are going with commodity hardware.  These don't have high end whizz bang components like the main frames mentioned above.  So they are going to fail -- and fail often.  We need to prepared for this.  How?
            </para>
            <para>
                The approach we will take is we build the 'intelligence' into the software.  So the cluster software will be smart enough to handle hardware failure.  The software detects hardware failures and takes corrective actions automatically -- without human intervention.  Our software will be smarter!
            </para>

            <bridgehead>
                Hadoop solution : Software is intelligent enough to deal with hardware failure
            </bridgehead>
        </section>

        <section>
            <title>
                Problem : hardware failure may lead to data loss
            </title>
            <para>
                So now we have a network of machines serving as a storage layer.  Data is spread out all over the nodes.  What happens when a node fails (and remember, we EXPECT nodes to fail).  All the data on that node will become unavailable (or lost).  So how do we prevent it?
            </para>
            <para>
                One approach is to make multiple copies of this data and store them on different machines.  So even if one node goes down, other nodes will have the data.  This is called 'replication'.  The standard replication is 3 copies.
            </para>
            <figure>
                <title>HDFS file replication</title>
                <graphic fileref="hdfs2.jpg"/>
            </figure>
            <bridgehead>
                Hadoop Solution : replicate (duplicate) data
            </bridgehead>
        </section>

        <section>
            <title>
                Problem : how will the distributed nodes co-ordinate among themselves
            </title>
            <para>
                Since each machine is part of the 'storage', we will have a 'daemon' running on each machine to manage storage for that machine.  These daemons will talk to each other to exchange data.
            </para>
            <para>
                Ok, now we have all these nodes storing data, how do we coordinate among them?  One approach is to have a MASTER to be the coordinator.  While building distributed systems with a centralized coordinator may seem like an odd idea, it is not a bad choice.  It simplifies architecture, design and implementation of the system
            </para>
            <para>
                So now our architecture looks like this.  We have a single master node and multiple worker nodes.
                <figure>
                    <title>HDFS master / worker design</title>
                    <graphic fileref="hdfs3.jpg"/>
                </figure>
            </para>
            <bridgehead>
                Hadoop solution : There is a master node that co-ordinates all the worker nodes
            </bridgehead>
        </section>
    </section>




    <sect1>
        <title>
            HDFS Architecture
        </title>
        <para>
            Now we have pretty much arrived at the architecture of HDFS
        </para>

        <figure>
            <title>HDFS architecture</title>
            <graphic fileref="hdfs4.jpg"/>
        </figure>
        <para>
            Lets go over some principles of HDFS.  First lets consider the parallels between 'our design' and the actual HDFS design.
        </para>

        <sect2>
            <title>Master / worker design</title>
            <para>
                In an HDFS cluster, there is ONE master node  and many worker nodes.  The master node is called the Name Node (NN) and the workers are called Data Nodes (DN).  Data nodes actually store the data.   They are the workhorses.
            </para>
            <para>
                Name Node is in charge of file system operations (like creating files, user permissions, etc.).   Without it, the cluster will be inoperable.  No one can write data or read data. <sbr/>
                This is called a Single Point of Failure.  We will look more into this later.
            </para>
        </sect2>

        <sect2>
            <title>Runs on commodity hardware</title>
            <para>
                As we saw hadoop doesn't need fancy, high end hardware.  It is designed to run on commodity hardware. The Hadoop stack is built to deal with hardware failure and the file system will continue to function even if nodes fail.
            </para>
        </sect2>

        <sect2>
            <title>HDFS is resilient (even in case of node failure)</title>
            <para>
                The file system will continue to function even if a node fails.  Hadoop accomplishes this by duplicating data across nodes.
            </para>
        </sect2>

        <sect2>
            <title>Data is replicated</title>
            <para>
                So how does Hadoop keep data safe and resilient in case of node failure?  Simple, it keeps multiple copies of data around the cluster.
            </para>
            <para>
                To understand how replication works, lets look at the following scenario.  Data segment #2 is replicated 3 times, on data nodes A, B and D.  Lets say data node A fails.  The data is still accessible from nodes B and D.
            </para>
        </sect2>

        <sect2>
            <title>HDFS is better suited for large files</title>
            <para>
                Generic file systems, say like Linux EXT file systems, will store files of varying size, from a few bytes to few gigabytes.  HDFS, however, is designed to store large files.  Large as in a few hundred megabytes to a few gigabytes.
            </para>
            <para>
                Why is this?
            </para>
            <para>
                HDFS was built to work with mechanical disk drives, whose capacity has gone up in recent years.  However, seek times haven't improved all that much.  So Hadoop tries to minimize disk seeks.
                <figure>
                    <title>Disk seek vs scan</title>
                    <graphic fileref="disk_seek.jpg"/>
                </figure>
            </para>
        </sect2>

        <sect2>
            <title>Files are write-once only (not updateable)</title>
            <para>
                HDFS supports writing files once (they cannot be updated).  This is a stark difference between HDFS and a generic file system (like a Linux file system).  Generic file systems allows files to be modified.
            </para>
            <para>
                However appending to a file is supported.  Appending is supported to enable applications like HBase.
                <figure>
                    <title>HDFS file append</title>
                    <graphic fileref="file_append.jpg"/>
                </figure>
            </para>
        </sect2>

    </sect1>

</chapter>
